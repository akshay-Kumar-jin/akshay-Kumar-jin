### Uncover the factors that lead to employee attrition.
### Show graphically using density charts:
### Show influence of different factors on attrition rate
### Which department is facing a major attrition problem.
### Show a breakdown of distance from home by job role and attritionâ€™
### compare average monthly income by education and attrition
### Provide few solutions to the problem of attrition at the company.

# to estimate the factors corresponding to Attrition rate of employees.

### Import the required packages
import pandas as pd
import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
#from sklearn.feature_extraction.text import CountVectorizer  #DT does not take strings as input for the model fit step....
from IPython.display import Image  
#import pydotplus as pydot
from sklearn import tree
from os import system
from sklearn.metrics import precision_score 
from sklearn.metrics import recall_score 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import f1_score 

df = pd.read_csv("Attrition Data.csv")

df.shape

df.head()

df.info() #as can be seen , no null data

corr = df.corr()
corr                      #no corr is greater than 0.8

# as can be seen, age, monthly income and being single have the highest effect on attrition rate

sns.heatmap(df.isnull(),yticklabels=False,cmap="viridis")

sns.countplot(x="Attrition",hue="Department",data=df) 

# # so,as can be seen the most attrition rate is in R&D ,followed by sales, and H.R                                                    

sns.countplot(x="DistanceFromHome",hue="Attrition",data=df) 

sns.countplot(x="DistanceFromHome",hue="JobRole",data=df) 

sns.countplot(x="MonthlyIncome",hue="Attrition",orient = "h", data=df) 

sns.countplot(x="MonthlyIncome",hue="Education",orient = "h", data=df) 

sns.countplot(x="Attrition",hue="BusinessTravel",data=df)

sns.countplot(x="Attrition",hue="EducationField",data=df)

sns.countplot(x="Attrition",hue="YearsWithCurrManager",data=df)

sns.countplot(x="Attrition",hue="Gender",data=df)

sns.countplot(x="Attrition",hue="MaritalStatus",data=df)

sns.countplot(x="Attrition",hue="RelationshipSatisfaction",data=df)

sns.countplot(x="Attrition",hue="JobSatisfaction",data=df)

sns.countplot(x="Attrition",hue="Education",data=df)

sns.countplot(x="Attrition",hue="OverTime",data=df)

df = df.drop('JobRole',axis = 1)
df.info()

## creating dummy variables to convert categorical datatypes to numerical
## and adding them to the list of columns

attrition =pd.get_dummies(df["Attrition"],prefix=['attrition'],drop_first=True)
attrition.head()

businessTravel= pd.get_dummies(df["BusinessTravel"],drop_first=True)
businessTravel.head()  #100 for non-travel

department= pd.get_dummies(df["Department"],drop_first=True)
department.head()  #100 for H.R

educationField = pd.get_dummies(df["EducationField"],drop_first=True)
educationField.head()  #00000 for H.R

gender = pd.get_dummies(df["Gender"],drop_first=True)
gender.head()  

maritalStatus = pd.get_dummies(df["MaritalStatus"],drop_first=True)
maritalStatus.head()  #001 for divorced

overTime = pd.get_dummies(df["OverTime"],prefix=['overtime'],drop_first=True)
overTime.head() 

df=pd.concat([df,attrition,businessTravel,overTime,maritalStatus,gender,educationField,department],axis=1)
df.head()

df.drop(["Attrition","BusinessTravel","OverTime","MaritalStatus","Gender","EducationField","Department"],axis=1,inplace=True)
df.head()   # Dropping the categorical columns now as they're unnecessary

df.info()

corr = df.corr()
corr         

sns.heatmap(corr,annot=True)

# splitting the data

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X=df.drop("['attrition']_Yes",axis=1)
y=df["['attrition']_Yes"]

X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=42)

# Logistic Regression

model=LogisticRegression()
model.fit(X_train,y_train)

predictions=model.predict(X_test)

from sklearn import metrics
cm=metrics.confusion_matrix(y_test, predictions, labels=[1, 0])

df_cm = pd.DataFrame(cm, index = [i for i in ["1","0"]],
                  columns = [i for i in ["Predict 1","Predict 0"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True,fmt='g')

#as can be seen the F.p are 10,and T.N are 56, where we should try to reduce T.N effectively

print(accuracy_score(y_test,predictions)*100)


print(precision_score(y_test,predictions)*100)
 

print(recall_score(y_test,predictions)*100)


print(f1_score(y_test,predictions)*100)

# DESICIONTREE

dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)
dTree.fit(X_train, y_train)

print(dTree.score(X_train, y_train))
print(dTree.score(X_test, y_test))

# as can be seen its overfitting

train_char_label = ['No', 'Yes']
Credit_Tree_File = open('credit_tree.dot','w')
dot_data = tree.export_graphviz(dTree, out_file=Credit_Tree_File, feature_names = list(X_train), class_names = list(train_char_label))
Credit_Tree_File.close()

# reduce overfitting (Regularization)

dTreeR = DecisionTreeClassifier(criterion = 'gini', max_depth = 4, random_state=1)
dTreeR.fit(X_train, y_train)        # curbing the no. of trees to 4

y_predict = dTreeR.predict(X_test)

cm=metrics.confusion_matrix(y_test, y_predict, labels=[0, 1])

df_cm = pd.DataFrame(cm, index = [i for i in ["No","Yes"]],
                  columns = [i for i in ["No","Yes"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True ,fmt='g')      # no. of T.N are 54

print(accuracy_score(y_test,y_predict)*100)

print(precision_score(y_test,y_predict)*100)

print(recall_score(y_test,y_predict)*100)

print(f1_score(y_test,y_predict)*100)

dTreeR1 = DecisionTreeClassifier(criterion = 'entropy', max_depth =5, random_state=1)
dTreeR1.fit(X_train, y_train) #checking with entropy, with depth 5

y_predict = dTreeR1.predict(X_test)

cm=metrics.confusion_matrix(y_test, y_predict, labels=[0, 1])

df_cm = pd.DataFrame(cm, index = [i for i in ["No","Yes"]],
                  columns = [i for i in ["No","Yes"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True ,fmt='g')      # no. of T.N are 54


print(accuracy_score(y_test,y_predict)*100)

print(precision_score(y_test,y_predict)*100)

print(recall_score(y_test,y_predict)*100)

print(f1_score(y_test,y_predict)*100)



# ENSEMBLE-Bagging

from sklearn.ensemble import BaggingClassifier

bgcl = BaggingClassifier(base_estimator=dTree,n_estimators=100,random_state=1)
#bgcl = BaggingClassifier(n_estimators=100,random_state=1)

bgcl = bgcl.fit(X_train, y_train)
y_predict = bgcl.predict(X_test)

cm=metrics.confusion_matrix(y_test, y_predict,labels=[0, 1])

df_cm = pd.DataFrame(cm, index = [i for i in ["No","Yes"]],
                  columns = [i for i in ["No","Yes"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True ,fmt='g')  #T.N reduced to 49 but F.P increased to 14

print(accuracy_score(y_test,y_predict)*100)

print(precision_score(y_test,y_predict)*100)

print(recall_score(y_test,y_predict)*100)

print(f1_score(y_test,y_predict)*100)

# ENSEMBLE - Ada boosting

from sklearn.ensemble import AdaBoostClassifier
abcl = AdaBoostClassifier(n_estimators=180, random_state=1)
                                     #abcl = AdaBoostClassifier( n_estimators=180,random_state=1)
abcl = abcl.fit(X_train, y_train)
                                    # no.of estimators =180 is the best choice.
y_predict = abcl.predict(X_test)

cm=metrics.confusion_matrix(y_test, y_predict,labels=[0, 1])

df_cm = pd.DataFrame(cm, index = [i for i in ["No","Yes"]],
                  columns = [i for i in ["No","Yes"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True ,fmt='g')   # #T.N reduced to 38 but F.P increased to 17

print(accuracy_score(y_test,y_predict)*100)

print(precision_score(y_test,y_predict)*100)

print(recall_score(y_test,y_predict)*100)

print(f1_score(y_test,y_predict)*100)

# ENSEMBLE - Gradient Boosting

from sklearn.ensemble import GradientBoostingClassifier
gbcl = GradientBoostingClassifier(n_estimators = 50,random_state=1)
gbcl = gbcl.fit(X_train, y_train)
y_predict = gbcl.predict(X_test)

cm=metrics.confusion_matrix(y_test, y_predict,labels=[0, 1])

df_cm = pd.DataFrame(cm, index = [i for i in ["No","Yes"]],
                  columns = [i for i in ["No","Yes"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True ,fmt='g') #T.N = 48, F.P = 9

print(accuracy_score(y_test,y_predict)*100)

print(precision_score(y_test,y_predict)*100)

print(recall_score(y_test,y_predict)*100)

print(f1_score(y_test,y_predict)*100)

# Ensemble RandomForest Classifier

from sklearn.ensemble import RandomForestClassifier
rfcl = RandomForestClassifier(n_estimators = 200, random_state=1,max_features=12)
rfcl = rfcl.fit(X_train, y_train)
y_predict = rfcl.predict(X_test)  # 12 no. of feauters is the best choice

cm=metrics.confusion_matrix(y_test, y_predict,labels=[0, 1])

df_cm = pd.DataFrame(cm, index = [i for i in ["No","Yes"]],
                  columns = [i for i in ["No","Yes"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True ,fmt='g')

print(accuracy_score(y_test,y_predict)*100)

print(precision_score(y_test,y_predict)*100)

print(recall_score(y_test,y_predict)*100)

print(f1_score(y_test,y_predict)*100)

# Since the cost of False negatives is high compared to false positives, Recall and F1-score are the suitable metrics to take, hence based on that Ada-boosting (with a recall of 37.7% and f1-score of 45.55%) as the data is also heavily biased towards true negatives, followed by Gradient boosting are the best classification models for us

# OBSERVATIONS and SUGGESTIONS to reduce Attrition rate

## for the company to reduce Attrition, it has to focus on it's R&D Department.. as they're facing highest Attrition rate

## and factors like Age,Monthly income and Being single play a key role..so, it  might help to give some pay-rises, and staff of high age have less attrition,so try to employ experienced staff..

## the male staff is facing a little higher Attrition rate than female staff


## as Environment satisfaction and Job satisfaction play a role too.., a proper complaint maintenance system for solving problems of work-environment..will definitley help to reduce Attrition

## and also,staff who travel rarely have a high Attrition rate,so travelling might help reduce Attrition..

## staff who work overtime are likely to face attrition,so distributing the work-load, might work effectively..

## and also, staff who have worked under the same manager for a long time..have a less Attrition rate.. so,mantaing the same teams,will be benificial..

## and other factors like Distance from home,no.of years in education,dont have a distinct pattern,but do play a role..



